<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="description" content="HiLEx is a large-scale dataset for Hierarchical Layout Extraction from exam question papers, with 1,965 annotated pages from 8 major exams. Includes hierarchical layout annotations (questions, answers, instructions, etc.), benchmarks with YOLO, Detectron2, DETR, and more." />
  <meta name="author" content="HiLEx Team" />
  <!-- Open Graph meta tags for social sharing -->
  <meta property="og:title" content="HiLEx Dataset: Hierarchical Layout Extraction" />
  <meta property="og:description" content="A benchmark dataset of exam question paper images with hierarchical layout annotations. 1,965 pages, 6 classes, YOLO/DETR benchmarks, ICDAR 2025." />
  <meta property="og:type" content="website" />
  <meta property="og:url" content="https://hilex-dla.github.io/HiLEx/" />
  <meta property="og:image" content="img/annotations_sample.jpg" />
  <meta property="og:image:alt" content="Annotated question paper layout example from HiLEx dataset" />
  <!-- Twitter Card tags (using large summary card) -->
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="HiLEx: Hierarchical Layout Extraction Dataset" />
  <meta name="twitter:description" content="1,965 exam pages with hierarchical layout annotations. Benchmark results and visualizations included." />
  <meta name="twitter:image" content="img/annotations_sample.jpg" />
  <title>HiLEx ‚Äì Hierarchical Layout Extraction Dataset</title>
  <!-- Google Font for clean sans-serif typography -->
  <link href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@400;600&display=swap" rel="stylesheet" />
  <style>
    /* Base styles and CSS Variables */
    :root {
      --primary-color: #4169e1; /* royal blue */
      --primary-color-dark: #325ad2; /* a darker shade for gradient */
      --bg-color: #fdfdfd;         /* pearl white background */
      --text-color: #333333;
      --max-width: 1200px;
    }
    * { margin: 0; padding: 0; box-sizing: border-box; }
    body {
      font-family: 'Open Sans', sans-serif;
      color: var(--text-color);
      background: var(--bg-color);
      line-height: 1.6;
    }
    header, nav, section, footer { width: 100%; }
    /* Sticky Navigation Bar */
    nav {
      position: fixed;
      top: 0; left: 0;
      width: 100%;
      background: transparent;
      display: flex;
      justify-content: center;
      align-items: center;
      padding: 0.5rem 1rem;
      z-index: 1000;
      transition: background 0.3s ease, box-shadow 0.3s ease;
    }
    nav.scrolled {
      background: #ffffff;
      box-shadow: 0 2px 5px rgba(0,0,0,0.1);
    }
    /* Navigation Branding (left side) */
    .brand {
      flex: 1;
      font-weight: 600;
      font-size: 1.1rem;
    }
    .brand a {
      color: #ffffff;
      text-decoration: none;
    }
    nav.scrolled .brand a {
      color: var(--primary-color);
    }
    /* Navigation Links (right side) */
    #navLinks {
      list-style: none;
      display: flex;
      gap: 1.5rem;
    }
    #navLinks li a {
      text-decoration: none;
      color: #ffffff;
      font-weight: 600;
      transition: color 0.3s;
    }
    /* Nav link hover effect */
    #navLinks li a:hover {
      text-decoration: underline;
    }
    nav.scrolled #navLinks li a {
      color: var(--primary-color);
    }
    /* Mobile menu toggle button */
    #navToggle {
      display: none;
      background: none;
      border: none;
      font-size: 1.5rem;
      color: #ffffff;
      cursor: pointer;
    }
    nav.scrolled #navToggle {
      color: var(--primary-color);
    }
    /* Responsive: show hamburger and hide links on small screens */
    @media (max-width: 768px) {
      #navLinks {
        display: none; /* hide links by default on mobile */
        flex-direction: column;
        background: #ffffff;
        position: absolute;
        top: 60px; right: 1rem;
        padding: 0.5rem 1rem;
        box-shadow: 0 2px 5px rgba(0,0,0,0.2);
      }
      #navLinks li {
        margin: 0.5rem 0;
      }
      #navLinks li a {
        color: var(--primary-color);
        font-weight: 600;
        text-decoration: none;
      }
      #navToggle {
        display: block;
        position: absolute;
        right: 1rem;
        /* top offset to vertically center with nav (approx) */
        top: 10px;
      }
      nav.open #navLinks {
        display: flex;
      }
    }
    /* Hero Section */
    #home {
      background: linear-gradient(to bottom, var(--primary-color) 0%, var(--primary-color-dark) 100%);
      color: #ffffff;
      padding: 6rem 1rem 4rem;
      text-align: center;
    }
    #home h1 {
      font-size: 2.5rem;
      font-weight: 700;
      margin-bottom: 0.5rem;
    }
    #home p.tagline {
      font-size: 1.2rem;
      margin-bottom: 1.5rem;
    }
    /* Section Containers */
    section {
      max-width: var(--max-width);
      margin: 0 auto;
      padding: 4rem 1rem;
    }
    section h2 {
      font-size: 2rem;
      color: var(--primary-color);
      margin-bottom: 1rem;
      text-align: center;
    }
    section h3 {
      font-size: 1.5rem;
      margin: 2rem 0 1rem;
      color: var(--primary-color);
    }
    section p {
      margin-bottom: 1rem;
    }
    /* About Section specific */
    #about h2::after {
      content: "";
      display: block;
      width: 60px;
      height: 4px;
      background: var(--primary-color);
      margin: 0.5rem auto 2rem;
      border-radius: 2px;
    }
    /* Dataset section lists */
    .stats-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
      gap: 1rem;
      margin: 1rem 0 2rem;
      text-align: center;
    }
    .stats-grid .stat-item {
      background: #f9f9ff;
      padding: 1rem;
      border: 1px solid #e0e0ef;
      border-radius: 8px;
    }
    .stat-item strong {
      display: block;
      font-size: 1.5rem;
      color: var(--primary-color);
    }
    /* Method model categories grid */
    .model-grid {
      display: flex;
      flex-wrap: wrap;
      gap: 2rem;
      margin-top: 1rem;
    }
    .model-family {
      flex: 1 1 calc(50% - 2rem); /* two per row on medium screens */
      background: #f9f9ff;
      border: 1px solid #e0e0ef;
      border-radius: 8px;
      padding: 1rem;
      min-width: 280px;
    }
    .model-family h3 {
      font-size: 1.2rem;
      margin-bottom: 0.5rem;
      color: var(--primary-color);
    }
    .model-family p {
      font-size: 0.95rem;
      margin-bottom: 0.5rem;
    }
    /* Results highlight list */
    .highlights {
      list-style: none;
      padding: 0;
      margin: 1rem 0 2rem;
    }
    .highlights li {
      margin: 0.5rem 0;
      padding-left: 1.5rem;
      position: relative;
    }
    .highlights li::before {
      content: "üèÜ";
      position: absolute;
      left: 0;
    }
    /* Tables (Leaderboard) */
    table {
      width: 100%;
      border-collapse: collapse;
      margin: 1rem 0;
      font-size: 0.95rem;
    }
    table, th, td {
      border: 1px solid #cccccc;
    }
    th, td {
      padding: 0.5rem;
      text-align: center;
    }
    th {
      background: #f0f0f8;
    }
    /* Make table horizontally scrollable on small screens */
    .table-container {
      overflow-x: auto;
    }
    /* Images and figures */
    figure {
      text-align: center;
      margin: 2rem 0;
    }
    figure img {
      max-width: 90%;
      height: auto;
      border: 1px solid #ccc;
    }
    figure figcaption {
      font-size: 0.9rem;
      color: #555555;
      margin-top: 0.5rem;
    }
    /* Citation block */
    pre.citation {
      background: #f8f8ff;
      padding: 1rem;
      overflow-x: auto;
      border: 1px solid #e0e0ef;
      border-radius: 4px;
      font-size: 0.9rem;
    }
    /* Contact info */
    .contact-info p {
      margin-bottom: 0.5rem;
    }
    .contact-info a {
      color: var(--primary-color);
      text-decoration: none;
    }
    .contact-info a:hover {
      text-decoration: underline;
    }
    /* Scroll-in reveal animations */
    .reveal { 
      opacity: 0;
      transform: translateY(20px);
      transition: opacity 0.6s ease, transform 0.6s ease;
    }
    .visible {
      opacity: 1;
      transform: translateY(0);
    }
  </style>
</head>
<body>
  <!-- Navigation Bar -->
  <nav>
    <div class="brand"><a href="#home">HiLEx</a></div>
    <button id="navToggle" aria-label="Toggle navigation menu">‚ò∞</button>
    <ul id="navLinks">
      <li><a href="#home">Home</a></li>
      <li><a href="#about">About</a></li>
      <li><a href="#dataset">Dataset</a></li>
      <li><a href="#method">Method</a></li>
      <li><a href="#results">Results</a></li>
      <li><a href="#visualizations">Visualizations</a></li>
      <li><a href="#paper">Paper</a></li>
      <li><a href="#leaderboard">Leaderboard</a></li>
      <li><a href="#citation">Citation</a></li>
      <li><a href="#contact">Contact</a></li>
    </ul>
  </nav>

  <!-- Home Hero Section -->
  <section id="home">
    <h1>HiLEx: Hierarchical Layout Extraction</h1>
    <p class="tagline">A large-scale benchmark for educational layout analysis</p>
    <p>Presented at ICDAR 2025 ‚Äì International Conference on Document Analysis and Recognition</p>
    <!-- Example call-to-action buttons (optional) -->
    <!--
    <a href="https://github.com/HiLEx-DLA/HiLEx" class="btn">View on GitHub</a>
    <a href="hilex_paper.pdf" class="btn">Download Paper (PDF)</a>
    -->
  </section>

  <!-- About Section -->
  <section id="about" class="reveal">
    <h2>About HiLEx</h2>
    <p>Document Layout Analysis (DLA) has advanced significantly in structured domains like invoices, forms, and academic papers. However, the layout understanding of educational content ‚Äì especially question papers ‚Äì remains largely underexplored. These documents are inherently multi-modal and hierarchical, containing a mix of textual structures like instructions, questions, answer blocks, and descriptions, often organized in complex multi-column formats.</p>
    <p><strong>HiLEx (Hierarchical Layout Extraction)</strong> is the first large-scale benchmark dataset designed specifically for understanding the layout of question paper images. It comprises 1,965 exam pages collected from eight major exams (GMAT, GRE, SAT, JEE, UPSC, GATE, BANK, and UGC-NET). Each page image is manually annotated with six hierarchical classes: <em>Question_Paper_Area</em>, <em>Question_Block</em>, <em>Answer_Block</em>, <em>Question_Answer_Block</em>, <em>Instruction</em>, and <em>Description</em>. Annotations are provided in both YOLO and COCO formats and were verified by expert annotators, achieving a gold-standard inter-annotator agreement (Cohen‚Äôs Œ∫ > 0.90).</p>
    <p>By enabling reliable layout extraction in educational contexts, HiLEx opens up opportunities for scalable document understanding, intelligent grading, and inclusive learning technologies. This aligns with broader goals of <a href="https://sdgs.un.org/goals/goal4" target="_blank" rel="noopener">SDG¬†4 (Quality Education)</a> and <a href="https://sdgs.un.org/goals/goal10" target="_blank" rel="noopener">SDG¬†10 (Reduced Inequalities)</a>.</p>
  </section>

  <!-- Dataset Section -->
  <section id="dataset" class="reveal">
    <h2>HiLEx Dataset</h2>
    <p>The HiLEx dataset is a curated benchmark for hierarchical layout analysis in educational documents, specifically designed for question papers that exhibit diverse and complex visual structures. It addresses a major gap in existing DLA benchmarks by targeting the education domain‚Äîan area rich in practical use cases like exam digitization, automated grading, and content retrieval.</p>
    <h3>Key Statistics</h3>
    <div class="stats-grid">
      <div class="stat-item">
        <strong>1,965</strong>
        Images
      </div>
      <div class="stat-item">
        <strong>8</strong>
        Exams Covered
      </div>
      <div class="stat-item">
        <strong>English</strong>
        Language
      </div>
      <div class="stat-item">
        <strong>6</strong>
        Layout Classes
      </div>
      <div class="stat-item">
        <strong>2</strong>
        Annotation Formats
      </div>
      <div class="stat-item">
        <strong>CC BY 4.0</strong>
        License
      </div>
    </div>
    <h3>Annotation Hierarchy</h3>
    <p>Each document image is annotated with a six-class hierarchical layout schema:</p>
    <ul>
      <li><strong>Question_Paper_Area:</strong> Full region containing the entire exam content.</li>
      <li><strong>Question_Answer_Block:</strong> Combined region containing both a question and its answer choices.</li>
      <li><strong>Question_Block:</strong> Question text only (excluding answer choices).</li>
      <li><strong>Answer_Block:</strong> Answer options (multiple choice options, numeric answers, etc.).</li>
      <li><strong>Instruction:</strong> Guidelines or section-level directions included in the paper.</li>
      <li><strong>Description:</strong> Explanatory content or solutions (e.g., an answer explanation segment).</li>
    </ul>
    <p>This hierarchy captures the structured and nested nature of question papers across both single-column and multi-column layouts.</p>
    <h3>Annotation Process</h3>
    <p>All annotations were performed manually by three domain experts, followed by multi-phase quality control. Final annotations achieved Cohen‚Äôs Œ∫ > 0.90, indicating very high inter-annotator agreement and reliability.</p>
    <h3>Data Distribution & Access</h3>
    <p>HiLEx includes exam content from both national and international assessments, supporting cross-cultural analysis. The dataset is publicly available under the <a href="https://creativecommons.org/licenses/by/4.0/" target="_blank" rel="noopener">Creative Commons Attribution 4.0 (CC BY 4.0)</a> license. You can access the data and tools from our GitHub repository:</p>
    <p><strong>GitHub:</strong> <a href="https://github.com/HiLEx-DLA/HiLEx" target="_blank" rel="noopener">HiLEx-DLA/HiLEx</a></p>
    <p>The repository provides:</p>
    <ul>
      <li><em>Images:</em> 1,965 question paper page images from 8 exams (listed above).</li>
      <li><em>Annotations:</em> Bounding boxes for the six layout classes, in both YOLO text files and COCO JSON format.</li>
      <li><em>Documentation:</em> A README with dataset details, class definitions, and sample visualization scripts.</li>
    </ul>
  </section>

  <!-- Method Section -->
  <section id="method" class="reveal">
    <h2>Method</h2>
    <p>To evaluate the effectiveness of the HiLEx dataset, we benchmarked a diverse set of object detection models spanning multiple architectural paradigms. Each model was trained to detect the six hierarchical layout categories within question paper images, enabling a thorough comparison of their capabilities on this task.</p>
    <h3>Model Categories</h3>
    <p>We grouped the evaluated models into four families:</p>
    <div class="model-grid">
      <div class="model-family">
        <h3>One-Stage Detectors</h3>
        <p>Fast, single-pass detectors that directly predict bounding boxes and classes in one go.</p>
        <p><em>Examples:</em> YOLOv8, YOLOv9, YOLOv10, YOLOv11, YOLOv12</p>
      </div>
      <div class="model-family">
        <h3>Two-Stage Detector</h3>
        <p>Region proposal-based model that first finds object regions, then classifies them.</p>
        <p><em>Example:</em> Detectron2 (Faster R-CNN backbone)</p>
      </div>
      <div class="model-family">
        <h3>Transformer-Based Models</h3>
        <p>Encoder-decoder models with object query mechanisms for global context object detection.</p>
        <p><em>Examples:</em> DETR, RT-DETR</p>
      </div>
      <div class="model-family">
        <h3>Vision-Language Models (VLMs)</h3>
        <p>Pre-trained multi-modal models that understand images and text jointly, allowing zero-shot layout detection.</p>
        <p><em>Examples:</em> Florence-2, PaLI-Gemma2</p>
      </div>
    </div>
    <h3>Implementation Details</h3>
    <p>All models were trained under consistent settings to ensure a fair comparison:</p>
    <ul>
      <li><strong>Platform:</strong> Google Colab with NVIDIA A100 GPU</li>
      <li><strong>Epochs:</strong> 25</li>
      <li><strong>Batch Size:</strong> 16</li>
      <li><strong>Optimizer:</strong> AdamW</li>
      <li><strong>Data Split:</strong> 80% Train / 10% Validation / 10% Test</li>
      <li><strong>Metrics:</strong> Precision, Recall, mAP@50, and mAP@50‚Äì95</li>
    </ul>
    <h3>Example Pipeline</h3>
    <p>During inference, each page image is processed by a trained model to detect hierarchical layout blocks (questions, answers, instructions, etc.). For YOLO-based detectors, we used Ultralytics YOLO pipelines; for transformer models like DETR, we leveraged HuggingFace and MMDetection frameworks. Model outputs were evaluated using IoU-based metrics and both per-class and overall averages.</p>
    <p>A typical evaluation pipeline included:</p>
    <ul>
      <li>Image pre-processing (resizing and normalization)</li>
      <li>Model inference to obtain bounding boxes for each class</li>
      <li>Metric computation (IoU, precision, recall for each class and overall)</li>
      <li>Error analysis and class-wise performance breakdown</li>
    </ul>
    <p>The results of this benchmarking are summarized below and on the Leaderboard, allowing comparison across model types and layout categories.</p>
  </section>

  <!-- Results Section -->
  <section id="results" class="reveal">
    <h2>Results</h2>
    <p>We evaluated HiLEx across a diverse set of models representing four major detection paradigms: one-stage, two-stage, transformer, and vision-language. All models were trained and tested on the same dataset split and evaluation criteria to ensure comparability.</p>
    <h3>Key Highlights</h3>
    <ul class="highlights">
      <li>Best overall performance: ü•á <strong>Detectron2</strong> with mAP@50 of 94.0% (two-stage fine-tuned detector)</li>
      <li>Strong one-stage performance: <strong>YOLOv8</strong> achieved mAP@50 of 84.8% with competitive speed and generalization</li>
      <li>Most balanced (high IoU) detection: <strong>YOLOv11</strong> had the highest mAP@50‚Äì95 (68.9%), indicating robust localization</li>
      <li>Vision-Language model promise: <strong>PaLI-Gemma2</strong> (zero-shot) reached 83.5% mAP@50, showing feasibility of layout detection without fine-tuning</li>
    </ul>
    <p>All models were evaluated on six layout classes with standard IoU thresholds (0.5 for mAP@50, and 0.5‚Äì0.95 for mAP@50‚Äì95). Detectron2, a two-stage model, attained the highest overall accuracy, while YOLO variants offered strong performance with lighter models. Vision-language models performed surprisingly well in zero-shot mode, though slightly behind fine-tuned models.</p>
    <h3>Class-wise Trends</h3>
    <p><em>Question_Paper_Area</em> and <em>Question_Answer_Block</em> were consistently detected with high precision across models, likely due to their large size and distinctive structure. In contrast, <em>Instruction</em> and <em>Description</em> were the most challenging classes ‚Äì many models struggled to detect these smaller, variably formatted elements, leading to lower recall for those classes. Notably, YOLOv11 excelled at localizing fine-grained Answer_Block regions, while PaLI-Gemma2 demonstrated strong generalization to unseen layouts despite not being fine-tuned.</p>
    <h3>Error Analysis</h3>
    <p>Common errors observed included over-detection in densely packed answer sections (multiple boxes around the same answer block) and missed instructions when their font style or placement differed from training examples. Vision-language models occasionally confused descriptions for question text blocks. These insights point to areas for future improvement, such as specialized handling of instruction text and better differentiation between similar text elements.</p>
  </section>

  <!-- Visualizations Section -->
  <section id="visualizations" class="reveal">
    <h2>Visualizations</h2>
    <p>HiLEx features richly annotated pages with diverse layouts ‚Äì from dense multi-column competitive exams to simpler single-question formats. Below we present key visual examples illustrating the dataset and model performance.</p>
    <h3>Ground Truth Layout Annotations</h3>
    <p>Each question paper image is annotated with color-coded boxes for the six structural classes, providing a hierarchical segmentation of the page content.</p>
    <figure>
      <img src="img/annotations_sample.jpg" alt="Sample HiLEx page with ground truth layout annotations" />
      <figcaption><em>Fig:</em> Example annotations on a question paper page, highlighting Question, Answer, Instruction, Description, etc.</figcaption>
    </figure>
    <p><em>Single-Column vs Multi-Column:</em> HiLEx includes both single-column papers and multi-column layouts. The annotations adapt to each style, capturing blocks appropriately in each format.</p>
    <h3>Model Predictions</h3>
    <p>Below are sample outputs from fine-tuned detection models on HiLEx pages. Detected layout components are drawn with class-specific colors.</p>
    <figure>
      <img src="img/yolo_predictions.jpg" alt="YOLOv8 model prediction on HiLEx page" />
      <figcaption><em>Fig:</em> YOLOv8 correctly identifies the Question region, Answer options, and overall Paper area on a sample page.</figcaption>
    </figure>
    <figure>
      <img src="img/detectron2_predictions.jpg" alt="Detectron2 model prediction on HiLEx page" />
      <figcaption><em>Fig:</em> Detectron2 output on the same page, successfully detecting even small Instruction elements that are easily overlooked.</figcaption>
    </figure>
    <h3>Performance Comparison</h3>
    <p>We visualize the detection performance across models to highlight accuracy vs. complexity trade-offs.</p>
    <figure>
      <img src="img/map_comparison.jpg" alt="Bar chart comparing mAP scores of all models on HiLEx" />
      <figcaption><em>Fig:</em> mAP@50 (blue) and mAP@50‚Äì95 (orange) for each evaluated model. Two-stage and one-stage models outperform transformer-based on higher IoUs, while VLMs approach one-stage performance in mAP@50.</figcaption>
    </figure>
    <h3>Failure Cases</h3>
    <p>Certain layout elements remained difficult. Two common failure modes are shown below: (1) missed instructions due to unusual formatting or placement, and (2) merged detections where question and answer text were not clearly separated.</p>
    <figure>
      <img src="img/error_analysis.jpg" alt="Examples of HiLEx model detection errors" />
      <figcaption><em>Fig:</em> Left: A model misses a faint instruction line (green) at the top of a question block. Right: An Answer_Block is over-detected, merging with neighboring text.</figcaption>
    </figure>
    <p>These visualizations help pinpoint where models perform well and where improvements are needed, guiding future research on hierarchical layout understanding.</p>
  </section>

  <!-- Leaderboard Section -->
  <section id="leaderboard" class="reveal">
    <h2>Leaderboard</h2>
    <p>The table below presents the benchmarking results of various models on the HiLEx dataset. Models are grouped by architecture type. We report performance using mAP@50 and mAP@50‚Äì95 (mean average precision at IoU 0.5 and 0.5:0.95, respectively), which are standard object detection metrics.</p>
    <p>All models were trained on the same training set and evaluated on an identical test set of HiLEx (covering all six layout classes). This ensures an apples-to-apples comparison of model capabilities.</p>
    <div class="table-container">
      <table>
        <tr>
          <th>Model</th>
          <th>Type</th>
          <th>Training</th>
          <th>Params</th>
          <th>mAP@50</th>
          <th>mAP@50‚Äì95</th>
        </tr>
        <tr>
          <td>Detectron2</td>
          <td>Two-Stage</td>
          <td>Fine-tuned</td>
          <td>~41M</td>
          <td>94.0%</td>
          <td>67.3%</td>
        </tr>
        <tr>
          <td>YOLOv8</td>
          <td>One-Stage</td>
          <td>Fine-tuned</td>
          <td>~35M</td>
          <td>84.8%</td>
          <td>66.1%</td>
        </tr>
        <tr>
          <td>YOLOv11</td>
          <td>One-Stage</td>
          <td>Fine-tuned</td>
          <td>~50M</td>
          <td>82.2%</td>
          <td>68.9%</td>
        </tr>
        <tr>
          <td>RT-DETR</td>
          <td>Transformer</td>
          <td>Fine-tuned</td>
          <td>~50M</td>
          <td>60.6%</td>
          <td>44.8%</td>
        </tr>
        <tr>
          <td>DETR</td>
          <td>Transformer</td>
          <td>Fine-tuned</td>
          <td>~86M</td>
          <td>50.2%</td>
          <td>35.0%</td>
        </tr>
        <tr>
          <td>PaLI-Gemma2</td>
          <td>Vision-Language</td>
          <td>Zero-shot</td>
          <td>&gt;1B</td>
          <td>83.5%</</td>
          <td>59.0%</td>
        </tr>
        <tr>
          <td>Florence-2</td>
          <td>Vision-Language</td>
          <td>Zero-shot</td>
          <td>&gt;1B</td>
          <td>81.0%</td>
          <td>59.0%</td>
        </tr>
      </table>
    </div>
    <p><small><em>Note:</em> All metrics are averaged across the six layout classes. Vision-Language models (Florence-2, PaLI-Gemma2) were evaluated in zero-shot mode (no fine-tuning on HiLEx).</small></p>
    <h3>Insights</h3>
    <ul>
      <li><strong>Most accurate class overall:</strong> Question_Paper_Area (easiest to detect due to its large size and page-border placement).</li>
      <li><strong>Most challenging classes:</strong> Instruction and Description (due to small size and varied formatting).</li>
      <li><strong>Top fine-tuned model:</strong> Detectron2 (highest mAP@50, excels at precise detection).</li>
      <li><strong>Best generalization:</strong> YOLOv11 (highest mAP@50‚Äì95, indicating good performance on stricter IoU).</li>
      <li><strong>Zero-shot leader:</strong> PaLI-Gemma2 (demonstrated strong results without any training on HiLEx).</li>
    </ul>
    <h3>Submission Instructions</h3>
    <p>If you evaluate a new model on HiLEx and wish to have it listed on the leaderboard:</p>
    <ol>
      <li>Fork the GitHub repository and add your results to the <code>leaderboard.json</code> (or relevant file).</li>
      <li>Include the model name, architecture type, parameter count, whether it was fine-tuned or zero-shot, and its mAP@50 and mAP@50‚Äì95 on HiLEx.</li>
      <li>Include a citation (BibTeX or URL) for your model/paper if applicable.</li>
      <li>Open a Pull Request with your changes. Our team will review it and, upon verification, merge it to update the official leaderboard.</li>
    </ol>
    <p>We encourage the community to test novel models on HiLEx ‚Äì let‚Äôs drive forward progress in document layout understanding!</p>
  </section>

  <!-- Paper Section -->
  <section id="paper" class="reveal">
    <h2>Paper</h2>
    <p><strong>HiLEx: Image-based Hierarchical Layout Extraction from Question Papers</strong><br>
       <em>Presented at the International Conference on Document Analysis and Recognition (ICDAR) 2025</em></p>
    <p><strong>Authors:</strong> [Names omitted for brevity]</p>
    <p>
      üìÑ <a href="https://arxiv.org/abs/xxxxx" target="_blank" rel="noopener">Preprint on arXiv</a> &nbsp; | &nbsp;
      üìò <a href="files/HiLEx_ICDAR2025.pdf" target="_blank" rel="noopener">PDF Download</a> &nbsp; | &nbsp;
      üíª <a href="https://github.com/HiLEx-DLA/HiLEx" target="_blank" rel="noopener">GitHub Repo</a>
    </p>
    <h3>Abstract</h3>
    <p>We introduce HiLEx, a new large-scale dataset and benchmark for hierarchical layout analysis in question paper images. HiLEx includes 1,965 exam pages annotated with six layout elements (questions, answers, instructions, etc.), covering content from eight diverse exams. We benchmark a range of models ‚Äî including YOLOv8‚Äì12, Detectron2, RT-DETR, Florence-2, and PaLI-Gemma2 ‚Äî and report state-of-the-art results using a two-stage fine-tuned detector. Our findings highlight the challenges of detecting instructions and descriptions, and the promise of vision-language models for future multi-modal layout tasks. This work aims to advance automated document understanding in education, supporting inclusive learning technologies aligned with SDG 4 and SDG 10.</p>
    <p><em>For full details and results, please refer to the paper and code linked above.</em></p>
  </section>

  <!-- Citation Section -->
  <section id="citation" class="reveal">
    <h2>Citation</h2>
    <p>If you use the HiLEx dataset, benchmarks, or tools in your research, please cite our paper:</p>
    <pre class="citation"><code>@inproceedings{hilex2025,
  title     = {{HiLEx}: Image-based Hierarchical Layout Extraction from Question Papers},
  author    = {Doe, John and Smith, Jane and et al.},
  booktitle = {Proc. of the Int. Conf. on Document Analysis and Recognition (ICDAR)},
  year      = {2025},
  url       = {https://github.com/HiLEx-DLA/HiLEx}
}</code></pre>
  </section>

  <!-- Contact Section -->
  <section id="contact" class="reveal">
    <h2>Contact</h2>
    <p>The HiLEx project is a collaborative effort by researchers working at the intersection of document analysis, computer vision, and educational AI. We welcome feedback and collaboration inquiries.</p>
    <div class="contact-info">
      <p><strong>Email:</strong> <a href="mailto:your.email@institution.edu">your.email@institution.edu</a></p>
      <p><strong>GitHub Issues:</strong> <a href="https://github.com/HiLEx-DLA/HiLEx/issues" target="_blank" rel="noopener">HiLEx-DLA/HiLEx</a></p>
      <p>Whether you are building educational AI tools, benchmarking layout models, or exploring document analysis in new domains ‚Äì we‚Äôd love to hear from you!</p>
    </div>
  </section>

  <!-- Footer (optional) -->
  <footer style="text-align:center; padding: 2rem 1rem; font-size: 0.9rem; color: #777;">
    ¬© 2025 HiLEx Project ‚Äì All Rights Reserved.
  </footer>

  <!-- JavaScript for interactivity -->
  <script>
    // Smooth scroll behavior is handled by CSS (scroll-behavior). Now handle nav color change on scroll:
    const nav = document.querySelector('nav');
    window.addEventListener('scroll', () => {
      if(window.pageYOffset > 50) {
        nav.classList.add('scrolled');
      } else {
        nav.classList.remove('scrolled');
      }
    });
    // Mobile nav toggle:
    const navToggle = document.getElementById('navToggle');
    navToggle.addEventListener('click', () => {
      // Toggle a class on nav to show/hide links
      nav.classList.toggle('open');
    });
    // Scroll reveal for sections:
    const revealElements = document.querySelectorAll('.reveal');
    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if(entry.isIntersecting) {
          entry.target.classList.add('visible');
          observer.unobserve(entry.target);
        }
      });
    }, { threshold: 0.1 });
    revealElements.forEach(el => observer.observe(el));
  </script>
</body>
</html>